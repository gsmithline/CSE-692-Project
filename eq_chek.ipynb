{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary cache directory at /var/folders/fh/fwc37qhn04d8sxp65hwv1kxm0000gn/T/matplotlib-7z3v6fvl because the default path (/Users/gabesmithline/.matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sys\n",
    "import os\n",
    "from nash_equilibrium.nash_solver import milp_max_sym_ent_2p, replicator_dynamics_nash, milp_nash_2p\n",
    "import pygambit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = \"/Users/gabesmithline/Desktop/caif_negotiation/test_matrices_performance_matrices/performance_matrix_0.csv\"\n",
    "game_matrix_2 = \"'meta_game_analysis/game_matrix_2_100_bootstrap/csv/performance_matrix.csv\"\n",
    "performance_matrix = pd.read_csv(matrix, index_col=0)\n",
    "payoff_matrix = performance_matrix.values\n",
    "agents = performance_matrix.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anthropic_3.7_sonnet_2025-02-19_circle_5</th>\n",
       "      <th>anthropic_3.7_sonnet_2025-02-19_circle_6</th>\n",
       "      <th>anthropic_sonnet_3.7_reasoning_2025-02-19_circle_0</th>\n",
       "      <th>gemini_2.0_flash_circle_2</th>\n",
       "      <th>gemini_2.0_flash_circle_5</th>\n",
       "      <th>openai_4o_2024-08-06_circle_4</th>\n",
       "      <th>openai_4o_2024-08-06_circle_5</th>\n",
       "      <th>openai_4o_2024-08-06_circle_6</th>\n",
       "      <th>openai_o3_mini_2025-01-31_circle_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>anthropic_3.7_sonnet_2025-02-19_circle_5</th>\n",
       "      <td>505.316907</td>\n",
       "      <td>389.841379</td>\n",
       "      <td>441.109434</td>\n",
       "      <td>586.527838</td>\n",
       "      <td>463.117742</td>\n",
       "      <td>535.820513</td>\n",
       "      <td>609.416279</td>\n",
       "      <td>539.964634</td>\n",
       "      <td>550.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthropic_3.7_sonnet_2025-02-19_circle_6</th>\n",
       "      <td>537.147241</td>\n",
       "      <td>554.136276</td>\n",
       "      <td>473.953171</td>\n",
       "      <td>495.094737</td>\n",
       "      <td>463.107941</td>\n",
       "      <td>663.525385</td>\n",
       "      <td>630.832000</td>\n",
       "      <td>602.463462</td>\n",
       "      <td>584.484750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthropic_sonnet_3.7_reasoning_2025-02-19_circle_0</th>\n",
       "      <td>542.563396</td>\n",
       "      <td>432.204146</td>\n",
       "      <td>496.895313</td>\n",
       "      <td>604.389111</td>\n",
       "      <td>467.763902</td>\n",
       "      <td>672.619706</td>\n",
       "      <td>454.183636</td>\n",
       "      <td>546.409111</td>\n",
       "      <td>593.853864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini_2.0_flash_circle_2</th>\n",
       "      <td>501.015946</td>\n",
       "      <td>592.210000</td>\n",
       "      <td>521.362222</td>\n",
       "      <td>494.653398</td>\n",
       "      <td>491.497872</td>\n",
       "      <td>451.561290</td>\n",
       "      <td>563.152143</td>\n",
       "      <td>582.493333</td>\n",
       "      <td>465.030222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini_2.0_flash_circle_5</th>\n",
       "      <td>539.252903</td>\n",
       "      <td>468.902647</td>\n",
       "      <td>564.572927</td>\n",
       "      <td>519.750638</td>\n",
       "      <td>532.880260</td>\n",
       "      <td>494.707073</td>\n",
       "      <td>523.029762</td>\n",
       "      <td>639.344500</td>\n",
       "      <td>588.016977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>openai_4o_2024-08-06_circle_4</th>\n",
       "      <td>480.110769</td>\n",
       "      <td>526.606667</td>\n",
       "      <td>427.662059</td>\n",
       "      <td>662.154839</td>\n",
       "      <td>552.726829</td>\n",
       "      <td>548.685053</td>\n",
       "      <td>658.030851</td>\n",
       "      <td>701.954054</td>\n",
       "      <td>510.547368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>openai_4o_2024-08-06_circle_5</th>\n",
       "      <td>592.361628</td>\n",
       "      <td>567.304222</td>\n",
       "      <td>577.080909</td>\n",
       "      <td>660.710714</td>\n",
       "      <td>465.343810</td>\n",
       "      <td>578.785745</td>\n",
       "      <td>611.763469</td>\n",
       "      <td>565.616364</td>\n",
       "      <td>497.661667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>openai_4o_2024-08-06_circle_6</th>\n",
       "      <td>646.404634</td>\n",
       "      <td>455.032692</td>\n",
       "      <td>532.803778</td>\n",
       "      <td>559.917667</td>\n",
       "      <td>446.255250</td>\n",
       "      <td>653.362973</td>\n",
       "      <td>582.700303</td>\n",
       "      <td>575.515625</td>\n",
       "      <td>597.430556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>openai_o3_mini_2025-01-31_circle_0</th>\n",
       "      <td>586.251111</td>\n",
       "      <td>520.210750</td>\n",
       "      <td>522.585455</td>\n",
       "      <td>567.686667</td>\n",
       "      <td>437.153023</td>\n",
       "      <td>548.840000</td>\n",
       "      <td>793.041389</td>\n",
       "      <td>628.189444</td>\n",
       "      <td>565.100500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    anthropic_3.7_sonnet_2025-02-19_circle_5  \\\n",
       "anthropic_3.7_sonnet_2025-02-19_circle_5                                          505.316907   \n",
       "anthropic_3.7_sonnet_2025-02-19_circle_6                                          537.147241   \n",
       "anthropic_sonnet_3.7_reasoning_2025-02-19_circle_0                                542.563396   \n",
       "gemini_2.0_flash_circle_2                                                         501.015946   \n",
       "gemini_2.0_flash_circle_5                                                         539.252903   \n",
       "openai_4o_2024-08-06_circle_4                                                     480.110769   \n",
       "openai_4o_2024-08-06_circle_5                                                     592.361628   \n",
       "openai_4o_2024-08-06_circle_6                                                     646.404634   \n",
       "openai_o3_mini_2025-01-31_circle_0                                                586.251111   \n",
       "\n",
       "                                                    anthropic_3.7_sonnet_2025-02-19_circle_6  \\\n",
       "anthropic_3.7_sonnet_2025-02-19_circle_5                                          389.841379   \n",
       "anthropic_3.7_sonnet_2025-02-19_circle_6                                          554.136276   \n",
       "anthropic_sonnet_3.7_reasoning_2025-02-19_circle_0                                432.204146   \n",
       "gemini_2.0_flash_circle_2                                                         592.210000   \n",
       "gemini_2.0_flash_circle_5                                                         468.902647   \n",
       "openai_4o_2024-08-06_circle_4                                                     526.606667   \n",
       "openai_4o_2024-08-06_circle_5                                                     567.304222   \n",
       "openai_4o_2024-08-06_circle_6                                                     455.032692   \n",
       "openai_o3_mini_2025-01-31_circle_0                                                520.210750   \n",
       "\n",
       "                                                    anthropic_sonnet_3.7_reasoning_2025-02-19_circle_0  \\\n",
       "anthropic_3.7_sonnet_2025-02-19_circle_5                                                   441.109434    \n",
       "anthropic_3.7_sonnet_2025-02-19_circle_6                                                   473.953171    \n",
       "anthropic_sonnet_3.7_reasoning_2025-02-19_circle_0                                         496.895313    \n",
       "gemini_2.0_flash_circle_2                                                                  521.362222    \n",
       "gemini_2.0_flash_circle_5                                                                  564.572927    \n",
       "openai_4o_2024-08-06_circle_4                                                              427.662059    \n",
       "openai_4o_2024-08-06_circle_5                                                              577.080909    \n",
       "openai_4o_2024-08-06_circle_6                                                              532.803778    \n",
       "openai_o3_mini_2025-01-31_circle_0                                                         522.585455    \n",
       "\n",
       "                                                    gemini_2.0_flash_circle_2  \\\n",
       "anthropic_3.7_sonnet_2025-02-19_circle_5                           586.527838   \n",
       "anthropic_3.7_sonnet_2025-02-19_circle_6                           495.094737   \n",
       "anthropic_sonnet_3.7_reasoning_2025-02-19_circle_0                 604.389111   \n",
       "gemini_2.0_flash_circle_2                                          494.653398   \n",
       "gemini_2.0_flash_circle_5                                          519.750638   \n",
       "openai_4o_2024-08-06_circle_4                                      662.154839   \n",
       "openai_4o_2024-08-06_circle_5                                      660.710714   \n",
       "openai_4o_2024-08-06_circle_6                                      559.917667   \n",
       "openai_o3_mini_2025-01-31_circle_0                                 567.686667   \n",
       "\n",
       "                                                    gemini_2.0_flash_circle_5  \\\n",
       "anthropic_3.7_sonnet_2025-02-19_circle_5                           463.117742   \n",
       "anthropic_3.7_sonnet_2025-02-19_circle_6                           463.107941   \n",
       "anthropic_sonnet_3.7_reasoning_2025-02-19_circle_0                 467.763902   \n",
       "gemini_2.0_flash_circle_2                                          491.497872   \n",
       "gemini_2.0_flash_circle_5                                          532.880260   \n",
       "openai_4o_2024-08-06_circle_4                                      552.726829   \n",
       "openai_4o_2024-08-06_circle_5                                      465.343810   \n",
       "openai_4o_2024-08-06_circle_6                                      446.255250   \n",
       "openai_o3_mini_2025-01-31_circle_0                                 437.153023   \n",
       "\n",
       "                                                    openai_4o_2024-08-06_circle_4  \\\n",
       "anthropic_3.7_sonnet_2025-02-19_circle_5                               535.820513   \n",
       "anthropic_3.7_sonnet_2025-02-19_circle_6                               663.525385   \n",
       "anthropic_sonnet_3.7_reasoning_2025-02-19_circle_0                     672.619706   \n",
       "gemini_2.0_flash_circle_2                                              451.561290   \n",
       "gemini_2.0_flash_circle_5                                              494.707073   \n",
       "openai_4o_2024-08-06_circle_4                                          548.685053   \n",
       "openai_4o_2024-08-06_circle_5                                          578.785745   \n",
       "openai_4o_2024-08-06_circle_6                                          653.362973   \n",
       "openai_o3_mini_2025-01-31_circle_0                                     548.840000   \n",
       "\n",
       "                                                    openai_4o_2024-08-06_circle_5  \\\n",
       "anthropic_3.7_sonnet_2025-02-19_circle_5                               609.416279   \n",
       "anthropic_3.7_sonnet_2025-02-19_circle_6                               630.832000   \n",
       "anthropic_sonnet_3.7_reasoning_2025-02-19_circle_0                     454.183636   \n",
       "gemini_2.0_flash_circle_2                                              563.152143   \n",
       "gemini_2.0_flash_circle_5                                              523.029762   \n",
       "openai_4o_2024-08-06_circle_4                                          658.030851   \n",
       "openai_4o_2024-08-06_circle_5                                          611.763469   \n",
       "openai_4o_2024-08-06_circle_6                                          582.700303   \n",
       "openai_o3_mini_2025-01-31_circle_0                                     793.041389   \n",
       "\n",
       "                                                    openai_4o_2024-08-06_circle_6  \\\n",
       "anthropic_3.7_sonnet_2025-02-19_circle_5                               539.964634   \n",
       "anthropic_3.7_sonnet_2025-02-19_circle_6                               602.463462   \n",
       "anthropic_sonnet_3.7_reasoning_2025-02-19_circle_0                     546.409111   \n",
       "gemini_2.0_flash_circle_2                                              582.493333   \n",
       "gemini_2.0_flash_circle_5                                              639.344500   \n",
       "openai_4o_2024-08-06_circle_4                                          701.954054   \n",
       "openai_4o_2024-08-06_circle_5                                          565.616364   \n",
       "openai_4o_2024-08-06_circle_6                                          575.515625   \n",
       "openai_o3_mini_2025-01-31_circle_0                                     628.189444   \n",
       "\n",
       "                                                    openai_o3_mini_2025-01-31_circle_0  \n",
       "anthropic_3.7_sonnet_2025-02-19_circle_5                                    550.555556  \n",
       "anthropic_3.7_sonnet_2025-02-19_circle_6                                    584.484750  \n",
       "anthropic_sonnet_3.7_reasoning_2025-02-19_circle_0                          593.853864  \n",
       "gemini_2.0_flash_circle_2                                                   465.030222  \n",
       "gemini_2.0_flash_circle_5                                                   588.016977  \n",
       "openai_4o_2024-08-06_circle_4                                               510.547368  \n",
       "openai_4o_2024-08-06_circle_5                                               497.661667  \n",
       "openai_4o_2024-08-06_circle_6                                               597.430556  \n",
       "openai_o3_mini_2025-01-31_circle_0                                          565.100500  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_regret(strategy, payoff_matrix):\n",
    "    \"\"\"\n",
    "    Compute the regret for each agent given a strategy.\n",
    "    \n",
    "    Args:\n",
    "        strategy: Nash equilibrium strategy vector\n",
    "        payoff_matrix: Game payoff matrix\n",
    "        \n",
    "    Returns:\n",
    "        regret: Vector of regrets for each agent\n",
    "        nash_value: Expected utility when Nash plays against itself\n",
    "    \"\"\"\n",
    "    # Expected utilities when playing against the Nash strategy\n",
    "    expected_utils = payoff_matrix @ strategy\n",
    "    \n",
    "    # Nash equilibrium value (expected utility when Nash plays against itself)\n",
    "    nash_value = strategy @ payoff_matrix @ strategy\n",
    "    \n",
    "    # Calculate regret for each agent\n",
    "    regret = expected_utils - nash_value\n",
    "    \n",
    "    return regret, nash_value, expected_utils\n",
    "\n",
    "def is_epsilon_nash(strategy, payoff_matrix, epsilon=0.05):\n",
    "    \"\"\"\n",
    "    Check if a strategy is an epsilon-Nash equilibrium.\n",
    "    \n",
    "    Args:\n",
    "        strategy: Strategy vector to check\n",
    "        payoff_matrix: Game payoff matrix\n",
    "        epsilon: Epsilon value for Nash equilibrium\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if it's an epsilon-Nash equilibrium\n",
    "    \"\"\"\n",
    "    regret, nash_value, _ = compute_regret(strategy, payoff_matrix)\n",
    "    max_regret = np.max(regret)\n",
    "    \n",
    "    print(f\"Maximum regret: {max_regret:.6f}\")\n",
    "    print(f\"Nash value: {nash_value:.6f}\")\n",
    "    \n",
    "    return max_regret <= epsilon, max_regret, nash_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Nash equilibria...\n",
      "Running nashpy replicator dynamics implementation...\n",
      "Running custom replicator dynamics implementation...\n",
      "  Custom reached max iterations with regret 32.330056 at start point 0\n",
      "  Custom reached max iterations with regret 141.087727 at start point 1\n",
      "  Custom reached max iterations with regret 38.073724 at start point 2\n",
      "  Custom reached max iterations with regret 80.185597 at start point 3\n",
      "  Custom reached max iterations with regret 167.501441 at start point 4\n",
      "  Custom reached max iterations with regret 19.846569 at start point 5\n",
      "  Custom reached max iterations with regret 123.934653 at start point 6\n",
      "  Custom reached max iterations with regret 181.277920 at start point 7\n",
      "  Custom reached max iterations with regret 126.438429 at start point 8\n",
      "  Custom reached max iterations with regret 32.330056 at start point 9\n",
      "  Custom reached max iterations with regret 32.330056 at start point 10\n",
      "  Custom reached max iterations with regret 32.330056 at start point 11\n",
      "  Custom reached max iterations with regret 126.438429 at start point 12\n",
      "  Custom reached max iterations with regret 126.438429 at start point 13\n",
      "  Custom reached max iterations with regret 123.934653 at start point 14\n",
      "  Custom reached max iterations with regret 126.438429 at start point 15\n",
      "  Custom reached max iterations with regret 123.934653 at start point 16\n",
      "  Custom reached max iterations with regret 32.330056 at start point 17\n",
      "  Custom reached max iterations with regret 123.934653 at start point 18\n",
      "  Custom reached max iterations with regret 32.330056 at start point 19\n",
      "Best nashpy regret: 0.862802\n",
      "Best custom regret: 11.963097\n",
      "Using nashpy implementation result (better regret)\n",
      "Best regret found: 0.862802 (above epsilon threshold)\n",
      "\n",
      "Replicator Dynamics Nash Equilibrium:\n",
      "                                               Agent   Probability\n",
      "0           anthropic_3.7_sonnet_2025-02-19_circle_5  8.813628e-10\n",
      "1           anthropic_3.7_sonnet_2025-02-19_circle_6  6.597473e-01\n",
      "2  anthropic_sonnet_3.7_reasoning_2025-02-19_circ...  8.813628e-10\n",
      "3                          gemini_2.0_flash_circle_2  2.306792e-02\n",
      "4                          gemini_2.0_flash_circle_5  8.813628e-10\n",
      "5                      openai_4o_2024-08-06_circle_4  1.138442e-07\n",
      "6                      openai_4o_2024-08-06_circle_5  1.477946e-01\n",
      "7                      openai_4o_2024-08-06_circle_6  8.813628e-10\n",
      "8                 openai_o3_mini_2025-01-31_circle_0  1.693900e-01\n",
      "\n",
      "Maximum Entropy Nash Equilibrium:\n",
      "                                               Agent   Probability\n",
      "0           anthropic_3.7_sonnet_2025-02-19_circle_5  1.814506e-14\n",
      "1           anthropic_3.7_sonnet_2025-02-19_circle_6  1.273825e-02\n",
      "2  anthropic_sonnet_3.7_reasoning_2025-02-19_circ...  2.299748e-01\n",
      "3                          gemini_2.0_flash_circle_2  1.978392e-14\n",
      "4                          gemini_2.0_flash_circle_5  3.321344e-01\n",
      "5                      openai_4o_2024-08-06_circle_4  2.849226e-01\n",
      "6                      openai_4o_2024-08-06_circle_5  1.281038e-14\n",
      "7                      openai_4o_2024-08-06_circle_6  1.402299e-01\n",
      "8                 openai_o3_mini_2025-01-31_circle_0  1.780393e-14\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing Nash equilibria...\")\n",
    "rd_nash, _ = replicator_dynamics_nash(payoff_matrix, max_iter=10000, epsilon=.05)\n",
    "\n",
    "\n",
    "\n",
    "me_nash = milp_max_sym_ent_2p(payoff_matrix)\n",
    "print(\"\\nReplicator Dynamics Nash Equilibrium:\")\n",
    "print(pd.DataFrame({\n",
    "    'Agent': agents,\n",
    "    'Probability': rd_nash\n",
    "}))\n",
    "\n",
    "print(\"\\nMaximum Entropy Nash Equilibrium:\")\n",
    "print(pd.DataFrame({\n",
    "    'Agent': agents,\n",
    "    'Probability': me_nash\n",
    "}))\n",
    "\n",
    "# Check if the equilibria are close to each other\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3839439816365484e-08"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "milp_strat = milp_nash_2p(performance_matrix, .05)\n",
    "milp_regret, _, _ = compute_regret(milp_strat, performance_matrix)\n",
    "max(milp_regret)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking if RD Nash is a 0.05-Nash equilibrium:\n",
      "Maximum regret: 0.862802\n",
      "Nash value: 568.387465\n",
      "RD Nash is a 0.05-Nash equilibrium: False\n",
      "\n",
      "Checking if ME Nash is a 0.05-Nash equilibrium:\n",
      "Maximum regret: 0.000000\n",
      "Nash value: 543.406878\n",
      "ME Nash is a 0.05-Nash equilibrium: True\n",
      "\n",
      "Regrets for each agent under RD Nash strategy:\n",
      "                                               Agent  Expected Utility  \\\n",
      "0           anthropic_3.7_sonnet_2025-02-19_circle_5        454.053912   \n",
      "1           anthropic_3.7_sonnet_2025-02-19_circle_6        569.250267   \n",
      "2  anthropic_sonnet_3.7_reasoning_2025-02-19_circ...        466.806421   \n",
      "3                          gemini_2.0_flash_circle_2        564.121976   \n",
      "4                          gemini_2.0_flash_circle_5        498.252083   \n",
      "5                      openai_4o_2024-08-06_circle_4        546.436988   \n",
      "6                      openai_4o_2024-08-06_circle_5        564.233002   \n",
      "7                      openai_4o_2024-08-06_circle_6        500.441557   \n",
      "8                 openai_o3_mini_2025-01-31_circle_0        569.232703   \n",
      "\n",
      "       Regret  \n",
      "0 -114.333553  \n",
      "1    0.862802  \n",
      "2 -101.581044  \n",
      "3   -4.265489  \n",
      "4  -70.135383  \n",
      "5  -21.950477  \n",
      "6   -4.154463  \n",
      "7  -67.945908  \n",
      "8    0.845238  \n",
      "\n",
      "Regrets for each agent under ME Nash strategy:\n",
      "                                               Agent  Expected Utility  \\\n",
      "0           anthropic_3.7_sonnet_2025-02-19_circle_5        488.613865   \n",
      "1           anthropic_3.7_sonnet_2025-02-19_circle_6        543.406878   \n",
      "2  anthropic_sonnet_3.7_reasoning_2025-02-19_circ...        543.406878   \n",
      "3                          gemini_2.0_flash_circle_2        501.030267   \n",
      "4                          gemini_2.0_flash_circle_5        543.406878   \n",
      "5                      openai_4o_2024-08-06_circle_4        543.406878   \n",
      "6                      openai_4o_2024-08-06_circle_5        538.722706   \n",
      "7                      openai_4o_2024-08-06_circle_6        543.406878   \n",
      "8                 openai_o3_mini_2025-01-31_circle_0        516.469501   \n",
      "\n",
      "         Regret  \n",
      "0 -5.479301e+01  \n",
      "1  7.958079e-12  \n",
      "2 -2.273737e-13  \n",
      "3 -4.237661e+01  \n",
      "4  6.934897e-12  \n",
      "5  4.547474e-13  \n",
      "6 -4.684172e+00  \n",
      "7  6.821210e-13  \n",
      "8 -2.693738e+01  \n",
      "\n",
      "Comparison of RD Nash and ME Nash:\n",
      "RD Nash Value: 568.387465\n",
      "ME Nash Value: 543.406878\n",
      "RD Nash Max Regret: 0.862802\n",
      "ME Nash Max Regret: 0.000000\n",
      "Difference in Nash Value: 24.980587\n",
      "Difference in Max Regret: 0.862802\n",
      "L1 distance between RD Nash and ME Nash: 1.974523\n"
     ]
    }
   ],
   "source": [
    "# Check if they are 0.05-Nash equilibria\n",
    "print(\"\\nChecking if RD Nash is a 0.05-Nash equilibrium:\")\n",
    "rd_is_nash, rd_max_regret, rd_value = is_epsilon_nash(rd_nash, payoff_matrix, 0.05)\n",
    "print(f\"RD Nash is a 0.05-Nash equilibrium: {rd_is_nash}\")\n",
    "\n",
    "print(\"\\nChecking if ME Nash is a 0.05-Nash equilibrium:\")\n",
    "me_is_nash, me_max_regret, me_value = is_epsilon_nash(me_nash, payoff_matrix, 0.05)\n",
    "print(f\"ME Nash is a 0.05-Nash equilibrium: {me_is_nash}\")\n",
    "\n",
    "print(\"\\nChecking if Plain MILP Nash is a 0.05-Nash equilibrium:\")\n",
    "milp_is_nash, milp_max_regret, milp_value = is_epsilon_nash(milp_strat, payoff_matrix, 0.05)\n",
    "print(f\"MILP Nash is a 0.05-Nash equilibrium: {me_is_nash}\")\n",
    "\n",
    "\n",
    "\n",
    "# Compute and display individual regrets for both strategies\n",
    "rd_regret, _, rd_expected_utils = compute_regret(rd_nash, payoff_matrix)\n",
    "me_regret, _, me_expected_utils = compute_regret(me_nash, payoff_matrix)\n",
    "milp_regret, _, milp_expected_utils = compute_regret(milp_strat, payoff_matrix)\n",
    "\n",
    "print(\"\\nRegrets for each agent under RD Nash strategy:\")\n",
    "print(pd.DataFrame({\n",
    "    'Agent': agents,\n",
    "    'Expected Utility': rd_expected_utils,\n",
    "    'Regret': rd_regret\n",
    "}))\n",
    "\n",
    "print(\"\\nRegrets for each agent under ME Nash strategy:\")\n",
    "print(pd.DataFrame({\n",
    "    'Agent': agents,\n",
    "    'Expected Utility': me_expected_utils,\n",
    "    'Regret': me_regret\n",
    "}))\n",
    "\n",
    "\n",
    "print(\"\\nRegrets for each agent under MILP Nash strategy:\")\n",
    "print(pd.DataFrame({\n",
    "    'Agent': agents,\n",
    "    'Expected Utility': milp_expected_utils,\n",
    "    'Regret': milp_regret\n",
    "}))\n",
    "\n",
    "\n",
    "\n",
    "# Compare the three equilibria\n",
    "''''\n",
    "print(\"\\nComparison of RD Nash and ME Nash:\")\n",
    "print(f\"RD Nash Value: {rd_value:.6f}\")\n",
    "print(f\"ME Nash Value: {me_value:.6f}\")\n",
    "print(f\"RD Nash Max Regret: {rd_max_regret:.6f}\")\n",
    "print(f\"ME Nash Max Regret: {me_max_regret:.6f}\")\n",
    "print(f\"Difference in Nash Value: {abs(rd_value - me_value):.6f}\")\n",
    "print(f\"Difference in Max Regret: {abs(rd_max_regret - me_max_regret):.6f}\")\n",
    "\n",
    "# Calculate the L1 distance between the two strategies\n",
    "l1_distance = np.sum(np.abs(rd_nash - me_nash))\n",
    "print(f\"L1 distance between RD Nash and ME Nash: {l1_distance:.6f}\")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
