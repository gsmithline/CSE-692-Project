{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary cache directory at /var/folders/fh/fwc37qhn04d8sxp65hwv1kxm0000gn/T/matplotlib-y_kd3p5r because the default path (/Users/gabesmithline/.matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sys\n",
    "import os\n",
    "from nash_equilibrium.nash_solver import milp_max_sym_ent_2p, replicator_dynamics_nash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_matrix = pd.read_csv('meta_game_analysis/game_matrix_2_100_bootstrap/csv/performance_matrix.csv', index_col=0)\n",
    "payoff_matrix = performance_matrix.values\n",
    "agents = performance_matrix.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anthropic_3.7_sonnet_circle_5</th>\n",
       "      <th>anthropic_3.7_sonnet_circle_6</th>\n",
       "      <th>anthropic_sonnet_3.7_reasoning_circle_0</th>\n",
       "      <th>gemini_2.0_flash_circle_2</th>\n",
       "      <th>gemini_2.0_flash_circle_5</th>\n",
       "      <th>openai_4o_circle_4</th>\n",
       "      <th>openai_4o_circle_5</th>\n",
       "      <th>openai_4o_circle_6</th>\n",
       "      <th>openai_o3_mini_circle_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>anthropic_3.7_sonnet_circle_5</th>\n",
       "      <td>613.562730</td>\n",
       "      <td>590.034520</td>\n",
       "      <td>589.555949</td>\n",
       "      <td>546.293220</td>\n",
       "      <td>476.773340</td>\n",
       "      <td>573.078226</td>\n",
       "      <td>632.128140</td>\n",
       "      <td>543.666267</td>\n",
       "      <td>665.612432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthropic_3.7_sonnet_circle_6</th>\n",
       "      <td>618.566720</td>\n",
       "      <td>556.448170</td>\n",
       "      <td>562.111340</td>\n",
       "      <td>555.239560</td>\n",
       "      <td>633.601560</td>\n",
       "      <td>475.911149</td>\n",
       "      <td>605.486238</td>\n",
       "      <td>632.358910</td>\n",
       "      <td>717.304041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthropic_sonnet_3.7_reasoning_circle_0</th>\n",
       "      <td>545.970369</td>\n",
       "      <td>606.590600</td>\n",
       "      <td>647.518860</td>\n",
       "      <td>479.915549</td>\n",
       "      <td>561.621450</td>\n",
       "      <td>574.660863</td>\n",
       "      <td>576.151356</td>\n",
       "      <td>564.651200</td>\n",
       "      <td>617.299950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini_2.0_flash_circle_2</th>\n",
       "      <td>457.088420</td>\n",
       "      <td>613.258060</td>\n",
       "      <td>649.005036</td>\n",
       "      <td>640.944330</td>\n",
       "      <td>614.174150</td>\n",
       "      <td>646.852567</td>\n",
       "      <td>518.866090</td>\n",
       "      <td>508.654636</td>\n",
       "      <td>558.485826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini_2.0_flash_circle_5</th>\n",
       "      <td>612.922890</td>\n",
       "      <td>518.026140</td>\n",
       "      <td>582.669180</td>\n",
       "      <td>534.727430</td>\n",
       "      <td>484.468240</td>\n",
       "      <td>552.947651</td>\n",
       "      <td>652.233979</td>\n",
       "      <td>547.774674</td>\n",
       "      <td>564.946173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>openai_4o_circle_4</th>\n",
       "      <td>530.045477</td>\n",
       "      <td>478.733056</td>\n",
       "      <td>514.871105</td>\n",
       "      <td>600.312367</td>\n",
       "      <td>609.980687</td>\n",
       "      <td>576.927565</td>\n",
       "      <td>636.923510</td>\n",
       "      <td>505.243795</td>\n",
       "      <td>606.505000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>openai_4o_circle_5</th>\n",
       "      <td>639.327440</td>\n",
       "      <td>594.107611</td>\n",
       "      <td>613.518589</td>\n",
       "      <td>547.360380</td>\n",
       "      <td>553.316103</td>\n",
       "      <td>582.837510</td>\n",
       "      <td>520.131150</td>\n",
       "      <td>542.836689</td>\n",
       "      <td>601.958421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>openai_4o_circle_6</th>\n",
       "      <td>716.942933</td>\n",
       "      <td>565.434510</td>\n",
       "      <td>606.004876</td>\n",
       "      <td>507.798862</td>\n",
       "      <td>563.803126</td>\n",
       "      <td>670.261323</td>\n",
       "      <td>582.806067</td>\n",
       "      <td>596.393667</td>\n",
       "      <td>600.586724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>openai_o3_mini_circle_0</th>\n",
       "      <td>630.508400</td>\n",
       "      <td>563.882441</td>\n",
       "      <td>654.336350</td>\n",
       "      <td>530.903456</td>\n",
       "      <td>633.336432</td>\n",
       "      <td>615.818063</td>\n",
       "      <td>665.244211</td>\n",
       "      <td>592.903849</td>\n",
       "      <td>624.193800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         anthropic_3.7_sonnet_circle_5  \\\n",
       "anthropic_3.7_sonnet_circle_5                               613.562730   \n",
       "anthropic_3.7_sonnet_circle_6                               618.566720   \n",
       "anthropic_sonnet_3.7_reasoning_circle_0                     545.970369   \n",
       "gemini_2.0_flash_circle_2                                   457.088420   \n",
       "gemini_2.0_flash_circle_5                                   612.922890   \n",
       "openai_4o_circle_4                                          530.045477   \n",
       "openai_4o_circle_5                                          639.327440   \n",
       "openai_4o_circle_6                                          716.942933   \n",
       "openai_o3_mini_circle_0                                     630.508400   \n",
       "\n",
       "                                         anthropic_3.7_sonnet_circle_6  \\\n",
       "anthropic_3.7_sonnet_circle_5                               590.034520   \n",
       "anthropic_3.7_sonnet_circle_6                               556.448170   \n",
       "anthropic_sonnet_3.7_reasoning_circle_0                     606.590600   \n",
       "gemini_2.0_flash_circle_2                                   613.258060   \n",
       "gemini_2.0_flash_circle_5                                   518.026140   \n",
       "openai_4o_circle_4                                          478.733056   \n",
       "openai_4o_circle_5                                          594.107611   \n",
       "openai_4o_circle_6                                          565.434510   \n",
       "openai_o3_mini_circle_0                                     563.882441   \n",
       "\n",
       "                                         anthropic_sonnet_3.7_reasoning_circle_0  \\\n",
       "anthropic_3.7_sonnet_circle_5                                         589.555949   \n",
       "anthropic_3.7_sonnet_circle_6                                         562.111340   \n",
       "anthropic_sonnet_3.7_reasoning_circle_0                               647.518860   \n",
       "gemini_2.0_flash_circle_2                                             649.005036   \n",
       "gemini_2.0_flash_circle_5                                             582.669180   \n",
       "openai_4o_circle_4                                                    514.871105   \n",
       "openai_4o_circle_5                                                    613.518589   \n",
       "openai_4o_circle_6                                                    606.004876   \n",
       "openai_o3_mini_circle_0                                               654.336350   \n",
       "\n",
       "                                         gemini_2.0_flash_circle_2  \\\n",
       "anthropic_3.7_sonnet_circle_5                           546.293220   \n",
       "anthropic_3.7_sonnet_circle_6                           555.239560   \n",
       "anthropic_sonnet_3.7_reasoning_circle_0                 479.915549   \n",
       "gemini_2.0_flash_circle_2                               640.944330   \n",
       "gemini_2.0_flash_circle_5                               534.727430   \n",
       "openai_4o_circle_4                                      600.312367   \n",
       "openai_4o_circle_5                                      547.360380   \n",
       "openai_4o_circle_6                                      507.798862   \n",
       "openai_o3_mini_circle_0                                 530.903456   \n",
       "\n",
       "                                         gemini_2.0_flash_circle_5  \\\n",
       "anthropic_3.7_sonnet_circle_5                           476.773340   \n",
       "anthropic_3.7_sonnet_circle_6                           633.601560   \n",
       "anthropic_sonnet_3.7_reasoning_circle_0                 561.621450   \n",
       "gemini_2.0_flash_circle_2                               614.174150   \n",
       "gemini_2.0_flash_circle_5                               484.468240   \n",
       "openai_4o_circle_4                                      609.980687   \n",
       "openai_4o_circle_5                                      553.316103   \n",
       "openai_4o_circle_6                                      563.803126   \n",
       "openai_o3_mini_circle_0                                 633.336432   \n",
       "\n",
       "                                         openai_4o_circle_4  \\\n",
       "anthropic_3.7_sonnet_circle_5                    573.078226   \n",
       "anthropic_3.7_sonnet_circle_6                    475.911149   \n",
       "anthropic_sonnet_3.7_reasoning_circle_0          574.660863   \n",
       "gemini_2.0_flash_circle_2                        646.852567   \n",
       "gemini_2.0_flash_circle_5                        552.947651   \n",
       "openai_4o_circle_4                               576.927565   \n",
       "openai_4o_circle_5                               582.837510   \n",
       "openai_4o_circle_6                               670.261323   \n",
       "openai_o3_mini_circle_0                          615.818063   \n",
       "\n",
       "                                         openai_4o_circle_5  \\\n",
       "anthropic_3.7_sonnet_circle_5                    632.128140   \n",
       "anthropic_3.7_sonnet_circle_6                    605.486238   \n",
       "anthropic_sonnet_3.7_reasoning_circle_0          576.151356   \n",
       "gemini_2.0_flash_circle_2                        518.866090   \n",
       "gemini_2.0_flash_circle_5                        652.233979   \n",
       "openai_4o_circle_4                               636.923510   \n",
       "openai_4o_circle_5                               520.131150   \n",
       "openai_4o_circle_6                               582.806067   \n",
       "openai_o3_mini_circle_0                          665.244211   \n",
       "\n",
       "                                         openai_4o_circle_6  \\\n",
       "anthropic_3.7_sonnet_circle_5                    543.666267   \n",
       "anthropic_3.7_sonnet_circle_6                    632.358910   \n",
       "anthropic_sonnet_3.7_reasoning_circle_0          564.651200   \n",
       "gemini_2.0_flash_circle_2                        508.654636   \n",
       "gemini_2.0_flash_circle_5                        547.774674   \n",
       "openai_4o_circle_4                               505.243795   \n",
       "openai_4o_circle_5                               542.836689   \n",
       "openai_4o_circle_6                               596.393667   \n",
       "openai_o3_mini_circle_0                          592.903849   \n",
       "\n",
       "                                         openai_o3_mini_circle_0  \n",
       "anthropic_3.7_sonnet_circle_5                         665.612432  \n",
       "anthropic_3.7_sonnet_circle_6                         717.304041  \n",
       "anthropic_sonnet_3.7_reasoning_circle_0               617.299950  \n",
       "gemini_2.0_flash_circle_2                             558.485826  \n",
       "gemini_2.0_flash_circle_5                             564.946173  \n",
       "openai_4o_circle_4                                    606.505000  \n",
       "openai_4o_circle_5                                    601.958421  \n",
       "openai_4o_circle_6                                    600.586724  \n",
       "openai_o3_mini_circle_0                               624.193800  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_regret(strategy, payoff_matrix):\n",
    "    \"\"\"\n",
    "    Compute the regret for each agent given a strategy.\n",
    "    \n",
    "    Args:\n",
    "        strategy: Nash equilibrium strategy vector\n",
    "        payoff_matrix: Game payoff matrix\n",
    "        \n",
    "    Returns:\n",
    "        regret: Vector of regrets for each agent\n",
    "        nash_value: Expected utility when Nash plays against itself\n",
    "    \"\"\"\n",
    "    # Expected utilities when playing against the Nash strategy\n",
    "    expected_utils = payoff_matrix @ strategy\n",
    "    \n",
    "    # Nash equilibrium value (expected utility when Nash plays against itself)\n",
    "    nash_value = strategy @ payoff_matrix @ strategy\n",
    "    \n",
    "    # Calculate regret for each agent\n",
    "    regret = expected_utils - nash_value\n",
    "    \n",
    "    return regret, nash_value, expected_utils\n",
    "\n",
    "def is_epsilon_nash(strategy, payoff_matrix, epsilon=0.05):\n",
    "    \"\"\"\n",
    "    Check if a strategy is an epsilon-Nash equilibrium.\n",
    "    \n",
    "    Args:\n",
    "        strategy: Strategy vector to check\n",
    "        payoff_matrix: Game payoff matrix\n",
    "        epsilon: Epsilon value for Nash equilibrium\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if it's an epsilon-Nash equilibrium\n",
    "    \"\"\"\n",
    "    regret, nash_value, _ = compute_regret(strategy, payoff_matrix)\n",
    "    max_regret = np.max(regret)\n",
    "    \n",
    "    print(f\"Maximum regret: {max_regret:.6f}\")\n",
    "    print(f\"Nash value: {nash_value:.6f}\")\n",
    "    \n",
    "    return max_regret <= epsilon, max_regret, nash_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Nash equilibria...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gabesmithline/Desktop/caif_negotiation/nash_equilibrium/nash_solver.py:252: RuntimeWarning: overflow encountered in exp\n",
      "  payoff_exp = np.exp(current_step_size * expected_payoffs)\n",
      "/Users/gabesmithline/Desktop/caif_negotiation/nash_equilibrium/nash_solver.py:257: RuntimeWarning: invalid value encountered in divide\n",
      "  new_strategy = new_strategy / np.sum(new_strategy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.47754352e-34 1.26567770e-32 1.16299888e-61 1.00000000e+00\n",
      " 6.52820284e-63 1.99213412e-49 6.75245897e-35 7.31319223e-52\n",
      " 2.00901999e-42]\n",
      "\n",
      "Replicator Dynamics Nash Equilibrium:\n",
      "                                     Agent   Probability\n",
      "0            anthropic_3.7_sonnet_circle_5  5.477544e-34\n",
      "1            anthropic_3.7_sonnet_circle_6  1.265678e-32\n",
      "2  anthropic_sonnet_3.7_reasoning_circle_0  1.162999e-61\n",
      "3                gemini_2.0_flash_circle_2  1.000000e+00\n",
      "4                gemini_2.0_flash_circle_5  6.528203e-63\n",
      "5                       openai_4o_circle_4  1.992134e-49\n",
      "6                       openai_4o_circle_5  6.752459e-35\n",
      "7                       openai_4o_circle_6  7.313192e-52\n",
      "8                  openai_o3_mini_circle_0  2.009020e-42\n",
      "\n",
      "Maximum Entropy Nash Equilibrium:\n",
      "                                     Agent   Probability\n",
      "0            anthropic_3.7_sonnet_circle_5  1.422996e-01\n",
      "1            anthropic_3.7_sonnet_circle_6  3.989840e-01\n",
      "2  anthropic_sonnet_3.7_reasoning_circle_0  1.800483e-01\n",
      "3                gemini_2.0_flash_circle_2  5.178436e-14\n",
      "4                gemini_2.0_flash_circle_5  4.120537e-14\n",
      "5                       openai_4o_circle_4  5.257892e-14\n",
      "6                       openai_4o_circle_5  5.123324e-14\n",
      "7                       openai_4o_circle_6  8.715725e-02\n",
      "8                  openai_o3_mini_circle_0  1.915108e-01\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing Nash equilibria...\")\n",
    "rd_nash = replicator_dynamics_nash(payoff_matrix, max_iter=10000, epsilon=.05)\n",
    "\n",
    "(print(rd_nash))\n",
    "\n",
    "me_nash = milp_max_sym_ent_2p(payoff_matrix)\n",
    "print(\"\\nReplicator Dynamics Nash Equilibrium:\")\n",
    "print(pd.DataFrame({\n",
    "    'Agent': agents,\n",
    "    'Probability': rd_nash\n",
    "}))\n",
    "\n",
    "print(\"\\nMaximum Entropy Nash Equilibrium:\")\n",
    "print(pd.DataFrame({\n",
    "    'Agent': agents,\n",
    "    'Probability': me_nash\n",
    "}))\n",
    "\n",
    "# Check if the equilibria are close to each other\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking if RD Nash is a 0.05-Nash equilibrium:\n",
      "Maximum regret: 0.000000\n",
      "Nash value: 640.944330\n",
      "RD Nash is a 0.05-Nash equilibrium: True\n",
      "\n",
      "Checking if ME Nash is a 0.05-Nash equilibrium:\n",
      "Maximum regret: 0.000000\n",
      "Nash value: 603.729065\n",
      "ME Nash is a 0.05-Nash equilibrium: True\n",
      "\n",
      "Regrets for each agent under RD Nash strategy:\n",
      "                                     Agent  Expected Utility      Regret\n",
      "0            anthropic_3.7_sonnet_circle_5        546.293220  -94.651110\n",
      "1            anthropic_3.7_sonnet_circle_6        555.239560  -85.704770\n",
      "2  anthropic_sonnet_3.7_reasoning_circle_0        479.915549 -161.028781\n",
      "3                gemini_2.0_flash_circle_2        640.944330    0.000000\n",
      "4                gemini_2.0_flash_circle_5        534.727430 -106.216900\n",
      "5                       openai_4o_circle_4        600.312367  -40.631963\n",
      "6                       openai_4o_circle_5        547.360380  -93.583950\n",
      "7                       openai_4o_circle_6        507.798862 -133.145468\n",
      "8                  openai_o3_mini_circle_0        530.903456 -110.040874\n",
      "\n",
      "Regrets for each agent under ME Nash strategy:\n",
      "                                     Agent  Expected Utility        Regret\n",
      "0            anthropic_3.7_sonnet_circle_5        603.729065 -1.364242e-12\n",
      "1            anthropic_3.7_sonnet_circle_6        603.729065  1.159606e-11\n",
      "2  anthropic_sonnet_3.7_reasoning_circle_0        603.729065  2.614797e-12\n",
      "3                gemini_2.0_flash_circle_2        577.864949 -2.586412e+01\n",
      "4                gemini_2.0_flash_circle_5        554.747273 -4.898179e+01\n",
      "5                       openai_4o_circle_4        519.321698 -8.440737e+01\n",
      "6                       openai_4o_circle_5        601.072170 -2.656895e+00\n",
      "7                       openai_4o_circle_6        603.729065  6.934897e-12\n",
      "8                  openai_o3_mini_circle_0        603.729065  1.216449e-11\n",
      "\n",
      "Comparison of RD Nash and ME Nash:\n",
      "RD Nash Value: 640.944330\n",
      "ME Nash Value: 603.729065\n",
      "RD Nash Max Regret: 0.000000\n",
      "ME Nash Max Regret: 0.000000\n",
      "Difference in Nash Value: 37.215265\n",
      "Difference in Max Regret: 0.000000\n",
      "L1 distance between RD Nash and ME Nash: 2.000000\n"
     ]
    }
   ],
   "source": [
    "# Check if they are 0.05-Nash equilibria\n",
    "print(\"\\nChecking if RD Nash is a 0.05-Nash equilibrium:\")\n",
    "rd_is_nash, rd_max_regret, rd_value = is_epsilon_nash(rd_nash, payoff_matrix, 0.05)\n",
    "print(f\"RD Nash is a 0.05-Nash equilibrium: {rd_is_nash}\")\n",
    "\n",
    "print(\"\\nChecking if ME Nash is a 0.05-Nash equilibrium:\")\n",
    "me_is_nash, me_max_regret, me_value = is_epsilon_nash(me_nash, payoff_matrix, 0.05)\n",
    "print(f\"ME Nash is a 0.05-Nash equilibrium: {me_is_nash}\")\n",
    "\n",
    "# Compute and display individual regrets for both strategies\n",
    "rd_regret, _, rd_expected_utils = compute_regret(rd_nash, payoff_matrix)\n",
    "me_regret, _, me_expected_utils = compute_regret(me_nash, payoff_matrix)\n",
    "\n",
    "print(\"\\nRegrets for each agent under RD Nash strategy:\")\n",
    "print(pd.DataFrame({\n",
    "    'Agent': agents,\n",
    "    'Expected Utility': rd_expected_utils,\n",
    "    'Regret': rd_regret\n",
    "}))\n",
    "\n",
    "print(\"\\nRegrets for each agent under ME Nash strategy:\")\n",
    "print(pd.DataFrame({\n",
    "    'Agent': agents,\n",
    "    'Expected Utility': me_expected_utils,\n",
    "    'Regret': me_regret\n",
    "}))\n",
    "\n",
    "# Compare the two equilibria\n",
    "print(\"\\nComparison of RD Nash and ME Nash:\")\n",
    "print(f\"RD Nash Value: {rd_value:.6f}\")\n",
    "print(f\"ME Nash Value: {me_value:.6f}\")\n",
    "print(f\"RD Nash Max Regret: {rd_max_regret:.6f}\")\n",
    "print(f\"ME Nash Max Regret: {me_max_regret:.6f}\")\n",
    "print(f\"Difference in Nash Value: {abs(rd_value - me_value):.6f}\")\n",
    "print(f\"Difference in Max Regret: {abs(rd_max_regret - me_max_regret):.6f}\")\n",
    "\n",
    "# Calculate the L1 distance between the two strategies\n",
    "l1_distance = np.sum(np.abs(rd_nash - me_nash))\n",
    "print(f\"L1 distance between RD Nash and ME Nash: {l1_distance:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
