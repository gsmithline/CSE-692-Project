{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install import_ipynb \n",
    "!conda install -c conda-forge import_ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from game_runner import NegotitaionGame\n",
    "from eval.game_evaluator import GameEvaluator\n",
    "import agents.simple_agent as simple_agent\n",
    "import agents.llm_agent as llm_agent\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass, field\n",
    "from math import prod, sqrt\n",
    "sys.path.append('../caif_negotiation/')\n",
    "\n",
    "# Import the notebook\n",
    "#import import_ipynb\n",
    "#%run '../test_game_eval.ipynb'  \n",
    "import torch\n",
    "from utils.offer import Offer\n",
    "\n",
    "from prompts.make_prompt import make_prompt\n",
    "from prompts.make_prompt_bargain import make_prompt_bargain\n",
    "from metrics.visualizations import (\n",
    "    plot_discounted_values,\n",
    "    plot_offer_evolution,\n",
    "    plot_negotiation_gap,\n",
    "    plot_fairness\n",
    ")\n",
    "import concurrent.futures\n",
    "\n",
    "\n",
    "pathology_results = pd.DataFrame()  \n",
    "import itertools\n",
    "envy_results_history = {}\n",
    "from eval.metrics import *\n",
    "from utils.helpers import *\n",
    "from utils.negotiation_game import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from math import sqrt, prod\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Configuration\n",
    "# ------------------------------------------------------------------------\n",
    "# prompt_style = 'llama_3.3_70b_maximize_value_outside_offer_cot_1_test'\n",
    "prompt_style = 'openai_o1_test_new_data_object_parallel'\n",
    "llm_type = 'openai'\n",
    "date = '1_28_2025'\n",
    "max_rounds = 3\n",
    "games = 10\n",
    "circles = [5, 6]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for circle in circles:\n",
    "    print(f\"Running game for circle {circle}\")\n",
    "    run_game(circle, games, max_rounds, date, prompt_style, llm_type)\n",
    "'''\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    future_to_circle = {\n",
    "        executor.submit(run_game, circle, games, max_rounds, date, prompt_style, llm_type): circle\n",
    "        for circle in circles\n",
    "    }\n",
    "    for future in concurrent.futures.as_completed(future_to_circle):\n",
    "        circle_val = future_to_circle[future]\n",
    "        try:\n",
    "            future.result()\n",
    "            print(f\"[INFO] circle={circle_val} run finished successfully.\")\n",
    "        except Exception as exc:\n",
    "            print(f\"[ERROR] circle={circle_val} generated an exception: {exc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml135_env_sp21",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
